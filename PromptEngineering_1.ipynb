{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering (Hands-on)\n",
    "\n",
    "In this lesson, you'll practice writing effective prompts for large language models. We will start with Open AI's GPT4 omni and gradually explore other prominent models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Load the API key and relevant Python libaries.\n",
    "\n",
    "Note: If you are a non-technical business user you don't need to understand the this section. Start from the Prompting Principle section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noobs way of instantiating the client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update your API key before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "# api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "client = OpenAI(api_key='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create your API keys from Open AI's website\n",
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A more sphisticated way to fetch the key would be to use environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "# import os\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv())\n",
    "# openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An even better way to do it if you are using Azure is to use Azure Key Vault\n",
    "I have a different repo that describes how to use Azure Key Vault"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Function\n",
    "We will create a basic helper function that will help us to focus on the prompting and not on more technical aspects. This function will take in a simple user prompt and return the response from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_completion(prompt: str, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"\n",
    "    Generates a completion for the given prompt using the specified AI model.\n",
    "\n",
    "    Args:\n",
    "    - prompt (str): The prompt to be sent to the AI model.\n",
    "    - model (str, optional): The model to use for generating the completion. Defaults to \"gpt-3.5-turbo\".\n",
    "\n",
    "    Returns:\n",
    "    - str: The generated completion text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Construct the message payload\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    \n",
    "    # Create the completion request\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0  # Controls the randomness of the output\n",
    "    )\n",
    "    \n",
    "    # Extract and return the content of the first choice\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Principles\n",
    "Getting Started: from asking it simple questions to writing clear and specific instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A straddle and a strangle are both options trading strategies that involve buying both a call option and a put option on the same underlying asset. The main difference between the two strategies is the strike prices of the options.\n",
      "\n",
      "To create a straddle:\n",
      "\n",
      "1. Choose an underlying asset that you believe will experience a significant price movement in the near future.\n",
      "2. Buy a call option with a specific strike price and expiration date.\n",
      "3. Buy a put option with the same strike price and expiration date as the call option.\n",
      "4. Pay the premiums for both options.\n",
      "\n",
      "The goal of a straddle is to profit from a significant price movement in either direction. If the price of the underlying asset moves significantly up or down, one of the options will become profitable while the other will expire worthless.\n",
      "\n",
      "To create a strangle:\n",
      "\n",
      "1. Choose an underlying asset that you believe will experience a moderate price movement in the near future.\n",
      "2. Buy a call option with a higher strike price and a put option with a lower strike price, both with the same expiration date.\n",
      "3. Pay the premiums for both options.\n",
      "\n",
      "The goal of a strangle is to profit from a moderate price movement in either direction. If the price of the underlying asset moves moderately up or down, one of the options will become profitable while the other will expire worthless.\n",
      "\n",
      "It is important to note that both straddles and strangles involve buying options, which means that there is a risk of losing the entire premium paid for the options if the price of the underlying asset does not move as expected. It is also important to carefully consider factors such as volatility, time decay, and the cost of the options when implementing these strategies.\n"
     ]
    }
   ],
   "source": [
    "# Lets begin at the begining by asking GPT to explain some well know concepts\n",
    "print (get_completion(\"Explain how to create a Straddle and a Strangle using Call and Put Options\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get a little serious and create a prompt that summarizes a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Developers are facing blocking issues due to development servers being down, potentially impacting a planned go-live tomorrow\n",
      "- Head of technology may ask everyone to work overtime if deadlines are not pushed\n",
      "- Colleague is asking for help in convincing the project sponsor to address the situation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "long_text = f\"\"\"\n",
    "This is a very long email from a colleague that was sent to you after office hours.\n",
    "There has been a new developement during the daily standup meeting. The developers are facing a blocking issues\n",
    "as the development servers are down. There is a planned go-live tomorrow, which may get impacted.\n",
    "The head of technology may have to ask everyone to work overtime if the deadlines are not pushed.\n",
    "Would you consider convincing the project sponsor?\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the text into two to three bullet points.\n",
    "```{long_text}```\n",
    "\"\"\"\n",
    "\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting structured results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\ '\n",
      "/tmp/ipykernel_26460/4148882899.py:6: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"book_id\": 1,\n",
      "        \"title\": \"The Agile Manifesto: A Practical Guide\",\n",
      "        \"author\": \"Samantha Reed\",\n",
      "        \"genre\": \"Non-fiction\"\n",
      "    },\n",
      "    {\n",
      "        \"book_id\": 2,\n",
      "        \"title\": \"Scrum Mastery: Leading Agile Teams\",\n",
      "        \"author\": \"Maxwell Greene\",\n",
      "        \"genre\": \"Business\"\n",
      "    },\n",
      "    {\n",
      "        \"book_id\": 3,\n",
      "        \"title\": \"Kanban Essentials: Streamlining Software Development\",\n",
      "        \"author\": \"Elena Chen\",\n",
      "        \"genre\": \"Technology\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Generate a list of three made-up book titles related to Agile software development along \\ \n",
    "with their authors and genres. \n",
    "Provide them in JSON format with the following keys: \n",
    "book_id, title, author, genre.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Product Owner>: Cash ratio is another liquidity ratio that can be calculated from the balance sheet. It measures a company's ability to cover its short-term liabilities with its cash and cash equivalents. To calculate the cash ratio, divide the total cash and cash equivalents by the current liabilities.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to answer in a consistent style.\n",
    "\n",
    "<Business Analyst>: Teach me about quick ratio.\n",
    "\n",
    "<Product Owner>: Quick ratio can be calculated from the balance sheet. \n",
    "Identify the Current Asset, subtarct the inventories and divide it by current liabilities\n",
    "\n",
    "<Business Analyst>: Teach me about cash ratio.\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of large language models (LLMs), hallucination refers to the generation of plausible-sounding content that is factually incorrect or nonsensical. This occurs when the model produces outputs that are not grounded in its training data or real-world knowledge. For example, an LLM might confidently state a false historical event or fabricate a scientific fact. Hallucination can be problematic in applications where accuracy and reliability are crucial, such as medical or legal contexts. Reducing hallucination involves improving the model's training data, refining its algorithms, and incorporating real-time verification mechanisms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Rhinoceros UltraSlim Smart Gargle Blaster by ChomChom is a cutting-edge gargle product designed to provide users with a refreshing and effective oral care experience. This innovative gargle blaster is equipped with advanced technology that helps to eliminate bacteria, freshen breath, and promote overall oral health.\n",
      "\n",
      "The Rhinoceros UltraSlim Smart Gargle Blaster features a sleek and compact design, making it easy to use and convenient to carry with you on the go. It is also equipped with a smart sensor that detects when the user is gargling and automatically dispenses the perfect amount of gargle solution for optimal results.\n",
      "\n",
      "This product is formulated with high-quality ingredients that are gentle on the mouth and provide long-lasting freshness. The Rhinoceros UltraSlim Smart Gargle Blaster is perfect for those looking for a convenient and effective way to maintain their oral hygiene routine.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Tell me about Rhinoceros UltraSlim Smart Gargle Blaster by ChomChom\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment analysis, when applied to large language models (LLMs), involves analyzing and interpreting the emotional tone or sentiment expressed in a piece of text. This can include identifying whether the sentiment is positive, negative, or neutral, and even detecting more nuanced emotions like joy, anger, or sadness.\n",
    "\n",
    "LLMs, like GPT, are trained on vast amounts of textual data and can understand the context, tone, and underlying emotions in text. By leveraging this capability, sentiment analysis can be performed in various applications, such as:\n",
    "\n",
    "Customer Feedback: Analyzing customer reviews or feedback to gauge satisfaction levels.\n",
    "\n",
    "Market Research: Understanding public opinion about a product, service, or brand on social media.\n",
    "\n",
    "Financial Analysis: Assessing the sentiment in financial news and reports to predict market trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sprint_review = \"\"\"\n",
    "The sprint went ok \\\n",
    "There were several challanges with requirements. \\\n",
    "The developers worked on adhic request \\\n",
    "But in the end we pulled through. \\\n",
    "Great work Team !!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the sprint review is positive. The team faced challenges with requirements but ultimately overcame them and completed the sprint successfully. The use of exclamation marks and the phrase \"Great work Team!!!\" indicate a sense of accomplishment and satisfaction with the team's efforts.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = f\"\"\"\n",
    "What is the sentiment of the following sprint review, \n",
    "which is delimited with triple backticks?\n",
    "\n",
    "Review text: '''{sprint_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok, challenges, worked, pulled through, great\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Identify a list of emotions that the writer of the \\\n",
    "following review is expressing. Include no more than \\\n",
    "five items in the list. Format your answer as a list of \\\n",
    "lower-case words separated by commas.\n",
    "\n",
    "Review text: '''{sprint_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Is the writer of the following review expressing anger?\\\n",
    "The review is delimited with triple backticks. \\\n",
    "Give your answer as either yes or no.\n",
    "\n",
    "Review text: '''{sprint_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to multimodal Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multimodal prompting for large language models (LLMs) refers to the integration of multiple types of data, such as text, images, audio, and video, to generate more comprehensive and contextually rich responses. This approach leverages the ability of LLMs to process and understand diverse data formats, enabling more sophisticated and nuanced interactions. \n",
    "In the following example we will ask questions pertaining to an uploaded image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='```json\\n{\\n  \"wearing_spectacles\": true,\\n  \"covering_face\": false\\n}\\n```', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "# from openai import OpenAI\n",
    "# # client = OpenAI()\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"/workspaces/ComplexPrompts/Arka1.jpeg\"\n",
    "\n",
    "# Getting the base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"Analyse the Image. Return a Json output that notifies whether the person is wearking spectacles or covering face\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hope you enjoyed learning and trying out the examples.\n",
    "See you in the future lessons.\n",
    "\n",
    "Arka"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
